\documentclass[a4paper,openany,12pt]{report}
\usepackage[french]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

%---PACKAGES-------------------------------------------------------------------
\usepackage[Lenny]{fncychap} % Lenny, Conny ,Bjarne, ejne, Glenn, Sonny
\usepackage{fancyhdr}
\usepackage{fancybox}
\usepackage{shadethm}
%\usepackage{slashbox}
\usepackage{eurosym}
\usepackage{lastpage}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb,mathtools,mathrsfs}
\usepackage{latexsym}
\usepackage[left=3cm, right=3cm, top=3cm, bottom=2cm]{geometry}
\usepackage{color,xcolor}
\usepackage{graphics}
\usepackage{float}
%\usepackage{setspace}
%\sim\usepackage{wrapfig}
%---SORTIES--------------------------------------------------------------------
\newif\ifpdf
\ifx\pdfoutput\undefined
   \pdffalse
\else
   \ifnum\pdfoutput=0
      \pdffalse
   \else
      \pdfoutput=1 \pdftrue
   \fi
\fi
%---PDF------------------------------------------------------------------------
\ifpdf
\usepackage[pdftex]{graphicx}
%\graphicspath{{images/}}
\DeclareGraphicsExtensions{.bmp,.jpg,.png}
\pdfcompresslevel=9
\usepackage[pdftex,                         % Param???trage de la navigation
bookmarks = true,                         % Signets
bookmarksnumbered = true,                 % Signets num???rot???s
pdfpagemode = None,                         % None, UseThumbs, UseOutlines, Fullscreen
pdfstartview = FitH,                         % FitH, FitV, FitR, FitB, FitBH, FitBV, Fit
pdfpagelayout = OneColumn,                 % SinglePage, OneColumn, TwoColumnLeft, TwoColumnRight
colorlinks = false,                         % Liens en couleur
urlcolor = black,                         % Couleur des liens externes
pdfborder = {0 0 0}                         % Style de bordure : ici, rien
]{hyperref}
\hypersetup{
pdfauthor = {},                                 % Auteurs
pdftitle = {},                 % Titre du document
pdfsubject = {},                                         % Sujet
pdfkeywords = {Math\'ematiques},                         % Mots-clefs
pdfcreator = {},                                         % Logiciel qui a cr???e le document
pdfproducer = {L'auteur}                                 % Soci???t??? avec produit le logiciel
plainpages = false}
\usepackage{pdfpages}
%---DVI------------------------------------------------------------------------
\else
\usepackage{graphicx}
\graphicspath{{eps/}}
\newcommand{\url}[1]{\emph{#1}}
\newcommand{\href}[2]{\emph{#2}[1]}
\fi
%----Macro-----------------
\newcommand{\KK}{\mathbb{K}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\FF}{\mathbb{F}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\MM}{\mathcal{M}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\gl}{\mathfrak{gl}}
\newcommand{\ssl}{\mathfrak{sl}}
\newcommand{\ttt}{\mathfrak{t}}
\newcommand{\nn}{\mathfrak{n}}
\newcommand{\dd}{\mathfrak{d}}
\newcommand{\ssp}{\mathfrak{sp}}
\newcommand{\oo}{\mathfrak{o}}
%---EN-TETE-ET-PIED-DE-PAGE----------------------------------------------------
\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{1 pt}
\pagestyle{fancy}
\lhead{}
%\chead{}
%\rhead{}
%\lfoot{}
%\cfoot{  \thepage\ }
%\rfoot{}
%---THEOREMES-------------------------------------------------------------------
\usepackage{ntheorem}
\theoremstyle{break}
{\theorembodyfont{\upshape}
\newtheorem*{rmq}{Remarque :}
\newtheorem*{prv}{Preuve :}
\newtheorem*{ex}{Exemples :}
\newtheorem*{exe}{Exemple : }
\newtheorem*{nota}{Notation :}
\newtheorem*{dem}{D\'emonstration :}}
\newshadetheorem{thm}{Th\'eor\`eme}[chapter]
\newshadetheorem{df}{D\'efinition}
\newshadetheorem{prop}{Proposition}[chapter]
\newshadetheorem{propr}{Propri\'et\'e}[chapter]
\newshadetheorem{lem}{Lemme}[chapter]
\newshadetheorem{cor}{Corollaire}[chapter]

%---PAGE-DE-GARDE--------------------------------------------------------------


\title{\textbf{Mémoire TER: \\ Sur les Algèbres de Lie Semisimples}}
\author{\bf Cassandre SAIZ \& Ezzahra ZLIGUI SAILLIER \\ \bf Encadré par Stéphane BASEILHAC \\ \\Université de Montpellier}

%\\ \\cassandre.saiz@etu.umontpellier.fr \\ ezzahra.zligui-saillier@etu.umontpellier.fr

\date{29/05/2020}

\begin{document} 

\maketitle
\thispagestyle{fancy}
\newpage
\vspace{4mm}
\clearpage
\tableofcontents
\clearpage

\begin{center}
\huge{\vspace*{2cm}\textbf{Introduction}}
\end{center}

%\quad Dans se rapport nous nous interreserons aux algèbres de Lie, plus particulièrement des algèbres de Lie semisimples.

\addcontentsline{toc}{chapter}{Introduction}
\vspace*{2cm}
    \newpage 



\chapter{Notions préliminaires }
Dans tout le rapport $\KK$ sera un corps commutatif et $V$ un $\KK$-espace vectoriel.

\section{Définitions et exemples}
\begin{df}
\quad Une $\KK \textbf{-algèbre de Lie}$ est un $\KK$-espace vectoriel $L$ muni d'une opération:
\begin{center}
$[. , . ]: L\times L\to  L,$\quad$(x,y) \to [x,y] $ 
\end{center}
appelée crochet de Lie et vérifiant: 
\begin{center}
\begin{itemize}
\item[(L1)] $[.,.]$ est bilinéaire.
\item[(L2)] $[x,x]=0$ pour tout $ x \in L $.
\item[(L3)] Identité de Jacobi:\quad
$[x,[y,z]]+[y,[z,x]]+[z,[x,y]]=0$, $\forall x,y,z \in L$.
\end{itemize}
\end{center}
\end{df}

\begin{rmq}
\quad (L1) et (L2) appliqué à $[x+y,x+y]$ implique l’anticommutativité, i.e. $[x,y]=-[y,x]$. De plus, si $car(\KK) \ne 2$ on a équivalence entre (L2) et $[x,y]=-[y,x]$, $\forall x,y \in L$.
\end{rmq}

\begin{ex}
\begin{enumerate}
\item Tout espace vectoriel muni du crochet nul, i.e . $[x,y]=0$, $\forall x,y \in L $, est une algèbre de Lie. Elle est appelée algèbre $\textbf{abélienne} $ ou $\textbf{commutative}$.

\item Si $V$ est un $\KK$-espace vectoriel,  alors $(End(V),+,\circ)$ muni du crochet : 
\begin{center}
$[x,y]=x \circ y-y \circ x$, $\forall (x,y) \in V \times V$
\end{center}
est une algèbre de Lie, on la note $\gl(V)$.

\item Si $ n \in \NN $, alors $(\MM_{n}(\KK),+,\times)$ muni du même crochet que ci-dessus est une algèbre de Lie, que l'on note $\gl(n,\KK)$.
\end{enumerate}
\end{ex}

\begin{df} 
\quad Soit $L$ une algèbre de Lie. On dit que $K$ est une \textbf{sous-algèbre de Lie} de $L$, si $K$ est un sous-espace vectoriel de $L$ et si pour tous $x , y \in K ,$ $[x,y] \in K$.
\end{df}

\subsection{Algèbres linéaires de Lie}\label{b}

Considérons $V$ un espace vectoriel de dimension $n$. Nous allons approfondir l'exemple $\gl(V)$, en étudiant certaines de ses sous-algèbres de Lie. Nous pouvons identifier $\gl(V)$  avec l'ensemble des matrices $n \times n$ sur $\KK$, que nous noterons $\gl(n,\KK)$. Cette procédure est pratique, surtout pour les calculs explicites. Comme pour le calcul de la dimension de $\gl(V)$ :
\begin{center}
$dim$ $\gl(V)= dim$ $\MM_{n}(\KK) = n^{2}$
\end{center}

Nous pouvons aussi définir une base de $\gl(n,\KK)$ : $ \{ e_{ij} \}$, où $e_{ij}$ est la matrice dont le seul coefficient non nul est sur la $i$-ième ligne et la $j$-ième colonne.
 
Nous avons donc $\gl(n,\KK)=vect\{ { e_{ij} \quad , 1\leq i , j \leq n }\}$. Regardons l'effet du crochet de Lie sur deux éléments de la base:
\begin{center}
$(\star)$ \quad $ [ e_{ij},e_{kl} ] = e_{ij} e_{kl} - e_{kl} e_{ij}  = \delta_{jk}e_{il}  -  \delta_{li}e_{kj} $, avec 
$ \delta_{ij} = \left \{
\begin{aligned}
1 & \quad si \quad i=j\\
0 & \quad sinon
\end{aligned}
\right. $
\end{center}

Toute sous-algèbre de Lie de $\gl(V)$ est appelée \emph{\textbf{algèbre linéaire de Lie}}. Nous allons voir 4 exemples importants  $A_{\ell}, B_{\ell}, C_{\ell}, D_{\ell}$.\\
\\
$ A_{\ell} $ : Dans un premier temps regardons l'\emph{\textbf{algèbre linéaire spéciale}}. Dans ce cas $dim$ $V= \ell +1$. L'algèbre linéaire spéciale, notée $\ssl(V)$ ou $\ssl(\ell+1,\KK)$, est l'ensemble des endomorphismes de $V$ dont la trace est nulle.

Rappelons que la trace d'une matrice est la somme de ses éléments diagonaux et qu'elle est indépendante du choix de la base. Comme nous pouvons identifier les matrices aux endomorphismes, la trace a du sens pour les endomorphismes de $V$.

Montrons que $\ssl(V)$ est bien une sous-algèbre de Lie. Comme $tr(xy)=tr(yx)$ et $tr(x+y)= tr(x) + tr(y)$,  pour tous $x,y  \in End(V)$ alors:
\begin{center}
$tr([x,y])=tr(xy-yx)=tr(xy)-tr(yx)=0$
\end{center} 
donc $[x,y] \in \ssl(V)$. D'où $\ssl(V)$ est bien une sous-algèbre de Lie.

Calculons la dimension de $\ssl(V)$, pour cela donnons une base de $\ssl(\ell+1,\KK)$:
\begin{center}
$\left \{
   \begin{array}{r c l}
e_{ij }\quad 1\leq i \neq j \leq \ell \\
e_{ii}-e_{i+1,i+1} \quad 1\leq i \leq \ell
   \end{array}
   \right .$
\end{center}

Nous pouvons alors déduire $dim$ $\ssl(V) = (\ell + 1)^2 - (\ell + 1) + \ell = \ell^2+2\ell$.\\
\\
Regardons le cas où $\ell=1$, c'est à dire $\ssl(2,\KK)$. Dans ce cas nous avons la base suivante:
\begin{center}
$ x=\begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix} $ ,\quad $ h=\begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix} $, \quad $ z=\begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix} $
\end{center}
\bigskip

Avant de commencer l'exemple $ C_{\ell} $, rappelons les définitions suivantes :

\begin{df} 
\quad Une \textbf{forme bilinéaire} $B$ d'un espace vectoriel $V$ sur $\KK$ est une application bilinéaire de $V\times V\to  \KK$.

On dit que $B$ est : 
\begin{itemize}
\item[•] \textbf{Symétrique} si $B(x,y)=B(y,x)$, $\forall x,y \in V$. 

\item[•] \textbf{Antisymétrique} si $B(x,y)=-B(y,x)$,  $\forall x,y \in V$. 

\item[•] \textbf{Non-dégénérée} si $\{ v \in V \mid B(v,w)=0$, $\forall w \in V \}= \{ 0 \}$.
\end{itemize}
\end{df}

\begin{rmq} 
\quad Dans les espaces vectoriels de dimension paire, si $B$ est une forme bilinéaire antisymétrique et non dégénérée, alors il existe une base $\mathcal{B}$ dans laquelle la matrice de $B$ est : $[ B ]_\mathcal{B}=\begin{pmatrix} 0 & I_\ell \\ -I_\ell & 0 \end{pmatrix} $.
\end{rmq}

$ C_{\ell} $: Parlons maintenant de l'\emph{\textbf{algèbre symplectique}}, dans ce cas $dim$ $V=2 \ell$. L'algèbre sympletique est définie comme l'ensemble des endomorphismes $x \in \gl(V)$ qui satisfont $f(x(v),w)=-f(v,x(w))$, pour $f$ une forme bilinéaire \textit{antisymétrique} non-dégénérée. On la note $\ssp (2\ell,\KK)$.

Montrons que $\ssp(2 \ell,\KK)$ est bien une sous-algèbre de $\gl(V)$. 
Soit $ x,y \in \ssp (V) $:
\begin{align*}
f([x,v]v ,w) &= f(xy(v),w)-f(yx(v),w) \\
           &= f(v,yx(w))-f(v,xy(w)) \\
           &= -(f(v,xy-yx(w) )\\
           &=-f(v,[x,y]x)
\end{align*} 
En terme matriciel:
\begin{center}
$\ssp (2 \ell,\KK)= \left \{ x\in \gl(2\ell,\KK) \mid sx=-x^ts, \quad s=\begin{pmatrix} 0 & I_\ell \\ -I_\ell & 0 \end{pmatrix} \right \}$.
\end{center}

Cherchons maintenant une base de $\ssp(2 \ell,\KK)$, pour cela nous allons décrire de façon plus précise les matrices symplectiques.

Soit $ x=\begin{pmatrix} m & n \\ p & q \end{pmatrix}$ avec $m,n,p,q \in \gl(\ell, \KK)$. Après calcul nous trouvons le système suivant : 
\[ \left\{
\begin{aligned}
n^t & = n \\
p^t & =p \\
 m^t & = -q
\end{aligned}
\right. \]

Il est alors facile de déduire des bases pour les espaces de $n$, $p$ et $\begin{pmatrix} m & 0 \\ 0 & q \end{pmatrix}$ qui sont respectivement: 
\begin{center}
$\left \{
   \begin{array}{r c l}
e_{i,i+\ell}\quad & 1\leq i \leq \ell \\
e_{i,j+\ell}+e_{j,i+\ell} & 1\leq i < j \leq \ell
   \end{array}
   \right  \}$
,$\left \{
   \begin{array}{r c l}
e_{i+\ell,i} &1\leq i \leq \ell \\
e_{i+\ell,j}+e_{j+\ell,i} & 1\leq i < j \leq \ell
   \end{array}
  \right   \} $
et 
\end{center}
\begin{center}
$\left \{
   \begin{array}{r c l}
e_{i,j}-e_{j+\ell,i+\ell} &1\leq i ,j \leq \ell
   \end{array}
\right   \}$.
\end{center}

Donc $dim$ $\ssp(2\ell,\KK) = \frac{2\ell(\ell+1)}{2} +\ell^2 = 2\ell^2+ \ell $.\\
\\
$ B_{\ell} $: On nomme \emph{\textbf{algèbre orthogonale}} impaire, notée  $\oo(2\ell+1,\KK)$, l'ensemble des $x \in \gl(V)$ tels que $f(x(v),w)=-f(v,x(w))$ pour $f$ une forme bilinéaire \textit{symétrique} non-dégénérée. Dans ce cas $dim$ $V=2\ell+1$ et $car(\KK) \ne 2$.

On reprend la même idée que pour $ C_{\ell} $:
\begin{center}
$\oo(2\ell+1,\KK)= \left \{ x \in \gl(2\ell+1, \KK) \mid sx= -x^ts,\quad s = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 0 & I_\ell \\ 0 & I_\ell & 0 \end{pmatrix} \right \}$.
\end{center}

Soit $x \in \oo(2 \ell +1, \KK)$, il sera alors de la forme:
\begin{center}
$x = 
\begin{pmatrix}
a & b_{1} & b_{2} \\
c_{1} & m & n \\
c_{2} & p & q 
\end{pmatrix} $
, avec $a \in \KK$, $b_{i}^t,c_{i} \in \KK^{\ell}$ et $m,n,p,q \in \gl(\ell,\KK)$.
\end{center}

En appliquant la condition $sx = -x^ts$ à $x$, on déduit le système suivant :
\[ \left \{
\begin{aligned}
a & = -a \\
c_{1} & = -b_{2}^t \\
c_{2} & = -b_{1}^t \\
q & = -m^t \\
p & = -p^t \\
n & = -n^t
\end{aligned}
\right. \]
On peut donc déduire qu'une base de $\oo(2\ell+1,\KK)$ est :

\[ \left \{
\begin{aligned}
e_{i,j}-e_{i+\ell,j+\ell } & \quad 2\leq i,j \leq \ell+1\\
e_{1,i+\ell}-e_{i,1} & \quad 2 \leq i \leq \ell+1 \\
e_{1,i}-e_{i+\ell,1} & \quad 2 \leq i \leq \ell+1 \\ 
e_{i,j+\ell}-e_{j,i+\ell} & \quad 2 \leq i < j \leq \ell+1\\
e_{i+\ell,j}-e_{j+\ell,i} & \quad 2 \leq i < j \leq \ell+1
\end{aligned}
\right \} \]
Alors $dim$ $\oo(2\ell+1,\KK)=2\ell^2+\ell$.\\
\\
$ D_{\ell} $:  On nomme l'\textbf{\emph{algèbre orthogonale}} paire, notée  $\oo (2\ell,\KK)$, l'ensemble:
\begin{center}
$ \left \{ x \in \gl(2\ell+1, \KK) \mid sx= -x^ts, \quad s =\begin{pmatrix} 0 & I_\ell \\ I_\ell & 0 \end{pmatrix} \right \}$
\end{center}
dans ce cas $dim$ $V=2 \ell$ et $car(\KK) \ne 2$.

Les calculs de base et de dimension se font de manière similaire à $B_{\ell}$. Nous donnerons donc la dimension de $\oo(2\ell,\KK)$ sans plus de précision, $dim$ $\oo(2\ell,\KK)=2\ell^2-\ell$.\\
\\
\quad Nous pouvons aussi cité comme sous-algèbre de Lie de $\gl(n,\KK)$:
\begin{itemize}
\item[•] Les matrices triangulaires supérieures $\ttt(n,\KK)$: $(a_{ij})= 0  \quad si \quad i>j.$

\item[•] Les matrices strictement  triangulaires  $ \nn(n,\KK)$: $(a_{ij})= 0  \quad si \quad i \geq j.$

\item[•] Les matrices diagonales $\dd(n,\KK)$: $(a_{ij})= 0  \quad si \quad i \ne j .$
\end{itemize}
et on a $\ttt(n,\KK)=\dd(n,\KK) \oplus \nn(n,\KK)$.

\subsection{Algèbres de Lie de  dérivation}

\quad Cette section nous est nécessaire pour indroduire la notion d'adjoint, notion qui nous sera utile dans la suite de ce rapport. C'est aussi le seul exemple de dérivation que nous verrons.

\begin{df}
\quad Une \textbf{$\KK$-algèbre} est un $\KK$-espace vectoriel, muni d'une opération bilinéaire $A \times A \to  A$
\end{df}

\begin{rmq}
\quad Une $\KK$-algèbre n'est pas nécessairement associative.
\end{rmq}

\begin{ex}
\begin{enumerate}
\item $A=End(V)$ \quad $x.y=x\circ y$.

\item $A=L$ une algèbre de Lie avec $x. y= [x,y]$. C'est l'exemple principal d'algèbre non-associative.
\end{enumerate}
\end{ex}

\begin{df}
\quad Une \textbf{dérivation} $\delta$ d'une $\KK$-algèbre $A$ est une application linéaire qui satisfait la formule de Leibniz:
\center $\delta(a.b)=a.\delta(b)+\delta(a).b$.
\end{df}

\begin{nota}
\quad On note $Der(A)=\{ $toutes les dérivations de $A \}$.
\end{nota}

\begin{prop}
\quad $Der(A)$ est un sous-espace vectoriel de $End(A)$.
De plus si $A$ est une algèbre de Lie, alors $Der(A)$ est une sous-algèbre de Lie. 
\end{prop}

\begin{prv}
Soit A une algèbre de Lie. Montrons que $Der(A)$ est une sous-algèbre de $\gl(A)$. Cela revient à montrer que pour tous $\delta ,\delta^{\prime} \in Der(A)$, on a:
\begin{center}
$[\delta,\delta^{\prime}] \in Der(A)$

c. à. d. \quad $[\delta,\delta^{\prime}](a.b)$ = $a.[\delta,\delta^{\prime}](b)+[\delta,\delta^{\prime}](a).b$ \quad $a,b\in A$.
\end{center}


Rappelons que $[\delta,\delta^{\prime}]=\delta\circ\delta^{\prime}-\delta^{\prime}\circ\delta$.

Soit $a,b\in A$:
\begin{align*}
[\delta,\delta^{\prime}](a.b) & = \delta\circ\delta^{\prime}-\delta^{\prime}\circ\delta(a.b) \\
& = \delta\circ\delta^{\prime}(a.b)-\delta^{\prime}\circ\delta(a.b)\\
& = \delta(a\delta^{\prime}(b)+\delta^{\prime}(a).b)-\delta^{\prime}(a\delta(b)+\delta(a).b)\\
& = \delta(a\delta^{\prime}(b))+\delta (\delta^{\prime}(a).b)-\delta^{\prime}(a\delta(b))-\delta^{\prime}(\delta(a).b)\\
& = a\delta\delta^{\prime}(b)+\delta(a)\delta^{\prime}(b)+\delta^{\prime}(a)\delta(b)+\delta\delta^{\prime}(a).b-\delta^{\prime}(a)\delta(b)-\delta(a)\delta^{\prime}(b)-\delta^{\prime}\delta(a).b \\
& = a(\delta\delta^{\prime}(b)-\delta^{\prime}\delta(b))+(\delta\delta^{\prime}(a)-\delta^{\prime}\delta(a)).b \\
& = a[\delta,\delta^{\prime}].b+[\delta,\delta^{\prime}](a).b\
\end{align*}
 $\Box$
\end{prv}

\begin{exe}
\quad Soit $L$ une algèbre de Lie. On appel \textbf{adjoint} de $x \in L$ l'application:
\begin{center}
$\begin{aligned} \text  ad \enskip x : L & \longrightarrow L \\ y & \longmapsto[x, y] \end{aligned}$
\end{center}

Montrons que l'adjoint est une dérivation. Soient $x,y,z \in L$ par l'identité de Jacobi on obtient:
\begin{center}
$[x,[y,z]]=-[y,[z,x]]-[z,[x,y]]=[y,[x,z]]+[[x,y],z].$
\end{center}
Pour simplifier l'écriture posons $a.b=[a,b]$, on a donc: 
\begin{center}
$ad$ $x(y.z)=y.ad$ $x(z)+ad$ $x(y).z$.
\end{center} 
Ainsi $ad$ $x$ est bien une dérivation.
\end{exe}

\section{Idéaux et homomorphismes}

\subsection{Idéaux}

\begin{df}
\quad Soit $L$ une algèbre de Lie. Soit $K_1$ et $K_2$ deux sous-algèbres de $L$. Le crochet $[K_1,K_2]$ est le sous-espace engendré par tous les commutateurs $[k_1,k_2]$, avec $k_1 \in K_1$ et $k_2 \in K_2$.
\end{df}

\begin{df}
\quad Soit $L$ une algèbre de Lie. On appel l'\textbf{algèbre dérivée} de $L$ le sous-espace $[L,L]$.
\end{df}

\begin{df}
\quad Un sous-ensemble $I$ de $L$ est un \textbf{idéal} de $L$ si $I$ est un sous-espace vectoriel de $L$ et si pour tous $x \in I$, $y \in L$, $[x, y] \in I$.
\end{df}

\begin{ex}
\begin{enumerate}
\item $\{0 \}$ , $L$ et $[L,L]$ sont des idéaux de $L$.

\item $Z(L) = \{ z \in L  \mid [x,z]=0 \quad \forall x \in L\}$ est appelé le \textbf{centre} de $L$. C'est un idéal de $L$.

\item Si $I$ et $J$ deux idéaux de $L$, alors $I+J:=\{i+j \mid i \in I$, $j \in J\}$ et $[I,J]$ sont des idéaux de $L$.

En effet pour $x \in L$, $i \in I$ et $j \in J$:
\begin{itemize}
\item[•] $[x,i+j]=[x,i]+[x,j]$ $\in I+J$
\item[•]$[x,[i,j]]=-[i,[j,x]]-[j,[x,i]]$ $\in [I,J]$ car $[x,i] \in I$, $[j,x] \in J$.\\ (Identité de Jacobi)
\end{itemize}
%(On a noté par [i,j] les éléments de [I,J] pour simplifier le calcul.)
\end{enumerate}
\end{ex}

\begin{df}
\quad Toute algèbre de Lie $L$ non abélienne est dite \textbf{simple} si ses seuls idéaux sont $\{ 0 \}$ et $L$.
\end{df}

\begin{exe}
\quad Soit $L = \ssl(2,\KK)$ avec $car(\KK)\ne 2$. On prend la base canonique de $\ssl(2,\KK)$ : 
\begin{center}
$\left \{ x=\begin{pmatrix} 0 & 1\\ 0 & 0 \end{pmatrix},\quad y=\begin{pmatrix} 0 & 0\\ 1 & 0 \end{pmatrix}\quad, h=\begin{pmatrix} 1 & 0\\ 0 & -1 \end{pmatrix} \right \}$.
\end{center} 

Par un simple calcul on trouve que : $[x,y]=h$,\quad $[h,x]=2x$, \quad $[h,y]=-2y$.

Soit $I \ne$ $\{0 \}$ un idéal de $L$ et $ax+by+ch$ un élément non nul de $I$. En appliquant  $ad$ $x$ deux fois à cet élément, on obtient $-2bx\in I$. De même en appliquant $ad$ $y$ deux fois, on obtient $-2ay \in I$. Donc si $a \ne 0$ et $b \ne 0$, alors $x \in I $ ou $y \in I$. Ainsi $I=L$.

Sinon, si $a = b = 0$ alors $ch \in I$, donc $h \in I$. On conclut $I=L$, donc $L$ est simple.
\end{exe}

Dans la section (\ref{d}) nous verrons des sous-algèbres de Lie qui ne sont pas simples, notamment les sous-algèbres $\ttt(n,\KK)$ et $\nn(n,\KK)$.\\
\\

La construction de l'algèbre $L / I$ est formellement le même que le quotient d'anneaux :

\begin{df}
\quad Soient $L$ une algèbre de Lie et $I$ un idéal propre et non nul de $L$. L'\textbf{algèbre quotient} $L/I$ est l'espace quotient muni de la multiplication de Lie définie par:
\center $[x+I, y+I]=[x, y]+I, \quad \forall(x, y) \in L \times L$
\end{df}

\begin{rmq}
\quad Dans le cas où $L$ n'est pas une algèbre de Lie simple et $dim$ $L > 1$, il est possible de factoriser par un idéal propre non nul. On obtient ainsi une algèbre de Lie de dimension plus petite.
\end{rmq}

Pour un usage ultérieur, nous allons introduire quelques notions connexes, analogues à celles de la théorie des groupes.

\begin{df}
\begin{itemize}

\item[•] Le \textbf{normalisateur} d'un sous-espace $K$ de $L$ est l'ensemble:  
 \begin{center}
$ N_{L}(K)=\{x \in L \mid [x,K] \subset K \} $ 
 \end{center}
Si $K = N_{L}(K)$ on dit que $K$ est un \textbf{auto-normalisateur}.

\item[•] Le \textbf{centralisateur} d'un sous-ensemble $S$ de $L$ est:
\begin{center}
$C_{L}(S) = \{x \in L \mid [x,S]=0\}$.
\end{center} 
\end{itemize}
\end{df}

\begin{rmq} 
\quad Par l’identité de Jacobi, $N_L(K)$ et $C_L(S)$ sont des sous-algèbres de $L$.

Dans le cas où $K$ est une sous-algèbre, $N_{L}(K)$ est la plus grande sous-algèbre de $L$ telle que $K$ soit un idéal.
\end{rmq}

\begin{ex}
\begin{enumerate}
\item Soient $L= \gl(n,\KK)$ et $K = \ssl(n,\KK)$. On a alors $N_L(K) = L$, car $\forall x \in L$, $\forall y \in K$, $tr([x,y]) = 0$, donc $[x,y] \in K$. De plus, comme $K$ une sous-algèbre de $L$, on a que $K$ est un idéal de $L$.

\item L'algèbre des matrices diagonales $ \dd(n,\CC) \subset \gl(n,\CC)$ est un exemple d'auto-normalisateur.

\item Soit $L$ une algèbre de Lie. Le centralisateur de $L$ est son centre, i.e. $C_L(L)=Z(L)$.

\end{enumerate}
\end{ex}

\subsection{Homomorphismes et représentations}

\begin{df}
Soient $L_{1}$, $L_{2}$ deux algèbres de Lie et $\phi: L_{1} \rightarrow L_{2}$ une application linéaire, on dit que $\phi$ est un \textbf{morphisme d'algèbres de Lie} (ou \textbf{homomorphisme}) si :
\begin{center}
$ \forall(x, y) \in L_{1} \times L_{1}, \quad \phi([x, y])=[\phi(x), \phi(y)]$.
\end{center}
De plus $\phi$ est:
\begin{itemize}
\item[•] un \textbf{monomorphisme} si $Ker$ $\phi = \{ 0 \}$.
\item[•] un \textbf{épimorphisme} si $Im$ $\phi = L_{2}$.
\item[•] un \textbf{isomorphisme} si $\phi$ est un épimorphisme et un monomorphisme.
\end{itemize}
$L_{1}$ et $L_{2}$ sont dites \textbf{isomorphes} s'il existe un isomorphisme d'algèbres de Lie $\phi: L_{1} \rightarrow L_{2}$.  
\end{df}

\bigskip
\begin{ex}
\begin{enumerate}

\item Le morphisme nul:
\begin{center} 
$\begin{aligned} \theta: L & \longmapsto \KK \\ x & \longmapsto 0 \end{aligned}$
\end{center}
Montrons que $\theta$ est un morphisme d'algèbres de Lie.

$\theta([x,y]) = 0 = [0,0] = [\theta(x),\theta(y)]$, donc $\theta$ est bien un morphisme.

\item La trace d'un endomorphisme est définie par: 
\begin{center}
$tr: \gl(n,\KK) \longrightarrow \KK$.
\end{center}
Montrons que $tr$ est un morphisme d’algèbres de Lie.

Soient $x,y \in  \gl(n,\KK)$. On a $tr([x,y]) = tr(xy-yx) = tr(xy)-tr(yx) = 0$
et $[tr(x),tr(y)] = tr(x)tr(y)-tr(y)tr(x) = 0$.

Donc la trace est bien un morphisme d'algèbres de Lie avec $Ker$ $tr = \ssl(n,\KK)$.

\item Le \textbf{morphisme canonique} est défini par:  
\begin{center}
$\begin{aligned} \pi: L & \longmapsto L/I \\ x & \longmapsto x+I\end{aligned}$
\end{center}
Montrons que $\pi$ est un épimorphisme d'algèbres de Lie.

On a $\pi([x,y]) = [x,y]+I$ et $[\pi(x),\pi(y)] = [x+I,y+I] = [x,y]+I$. Donc $\pi([x,y]) = [\pi(x),\pi(y)]$. De plus, on a par définition $Im$ $\pi = L/I$ et $Ker$ $\pi = I$.

Donc $\pi$ est bien un épimorphisme.
\end{enumerate}
\end{ex}

\begin{rmq}
\quad Soient $L_{1}$ et $L_{2}$ des algèbres de Lie. Si $\phi$ est un morphisme de $L_{1}$ dans $L_{2}$, alors :
\begin{itemize}
\item[•] $Ker$ $\phi$ est un idéale de $L_{1}$.
\item[•] $Im$ $\phi$ est une sous-algèbre de $L_{2}$.
\end{itemize}
\end{rmq}

\quad Nous allons maintenant énoncer le théorème standard d'isomorphisme.

\begin{thm}\label{thm:iso}
Soit $\phi: L_{1}  \to L_{2} $ un homomorphisme d'algèbre de Lie. Et soient $I$ et $J$ deux idéaux de $L_{1}$.
Alors: 
\begin{itemize}
\item[(a)] $L / Ker$ $\phi \cong Im$ $\phi$.
\item[(b)] Si $I \subset Ker$ $\phi$, alors il existe un unique homomorphisme 

$\psi: L_{1} / I \to L_{2} $ tel que $\psi \circ \pi= \phi$.
\item[(c)] Si $I \subset J$ alors $J / I$ est un idéal de $L/ I$ et $(L_{1} / I) /(J / I)\cong L_{1} / J$.
\item[(d)] $(I+J) / J \cong I /(I \cap J)$.
\end{itemize}
\end{thm}

\begin{df}
\quad  Une \textbf{représentation} d’une algèbre de Lie $L$ est un homomorphisme \\ $\phi: L \rightarrow \gl(V)$, où $V$ est un $\KK$-espace vectoriel.

Si $\phi$ est injectif, on dit que c'est une représentation  \textbf{fidèle}.
\end{df}

\quad Bien que nous ayons besoin que $L$ soit de dimension finie, il est utile de permettre à $V$ d'être de dimension quelconque. Noter que $\gl(V)$ à du sens en toute dimension.

\begin{ex}
\begin{enumerate}

\item La \textbf{représentation triviale} de $L$ dans $V$ est l'application $\phi$ définie par $\forall x \in L$, $\phi(x)=0$.

\item La \textbf{représentation adjointe} est l'application:
\begin{center}
$
\begin{aligned}
ad : L & \longrightarrow \gl(V) \\
x & \longmapsto ad \enskip x 
\end{aligned}$
\end{center}
Il est clair qu'elle est linéaire, montrons qu'elle préserve le crochet:
\[
\begin{aligned}
[ad \enskip x, ad \enskip y](z) & = ad \enskip x \enskip ad \enskip y(z)- ad \enskip y \enskip ad \enskip x(z) \\
& = ad \enskip x([y,z])-ad \enskip y([x,z]) \\
& = [x,[y,z]]-[y,[x,z]] \\
& = [x,[y,z]]+[[x,z],y] \\
& = [[x,y],z] \\
& = ad \enskip [x,y](z)
\end{aligned} \]
Soit $x \in L$ tel que $ad$ $x = 0$, donc $Ker$ $ad = Z(L)$. 

Cette constatation nous permet de déduire:
\medskip

\quad Si $L$ est simple alors $ad: L \longrightarrow \gl(L)$ est un monomorphisme, ainsi toute algèbre de Lie simple est isomorphe à une algèbre de Lie linéaire.
\end{enumerate}
\end{ex}

\section{Algèbres de Lie résolubles et nilpotentes}

\subsection{Algèbres résolubles}\label{d}

\quad Il est naturel pour étudier les algèbres de Lie d'étudier leurs idéaux. En premier on définit une suite d'idéaux de L, que l'on appel \textbf{série dérivée}, par:
\begin{center}
$L^{(0)} = L$, $L^{(1)} = [L,L]$, $L^{(2)} = [L^{(1)},L^{(1)}] \cdots L^{(i)} = [L^{(i-1)},L^{(i-1)}]$.
\end{center}

Justifions que $L^{(i)}$ est un idéal $\forall i$:

\quad Par réccurence, $L^{(0)} = L$ est évidament un idéal de $L$. Supposons que $L^{(i)}$ est un idéal de $L$. Montrons que $L^{(i+1)}$ est encore un idéal. Soit $x \in L$ et $[y,z] \in L^{(i+1)}$, avec $y,z \in L^{(i)}$:
\begin{align*}
[x,[y,z]] & = -[y,[z,x]]-[z,[x,y]]
\end{align*}
or comme $L^{(i)}$ est un idéal, on a $[z,x],[x,y] \in L^{(i)} \Rightarrow [y,[z,x]],[z,[x,y]] \in L^{(i)}$, donc $[x,[y,z]] \in L^{(i+1)}$. De plus, il est clair que $L^{(i+1)}$ est une sous-algèbre de $L$.

Ainsi $L^{(i+1)}$ est un idéal de $L$. 

\begin{df}
\quad Une algèbre de Lie $L$ est dite \textbf{résoluble} si: $\exists n \in \NN $ tel que $L^{(n)} = \{0\}$.
\end{df}

\begin{rmq}
\begin{itemize}
\item[•]  $L$ abélien $\Rightarrow L$ résoluble. En effet, $L$ abélienne équivaut à $L^{(1)} = \{0\}$, donc $L$ est résoluble.

\item[•]  $L$ simple $\Rightarrow L$ n'est pas résoluble.

En effet: on a  $L$ est simple donc $L$ n'est pas  abélien et n'a pas d'ideaux propre différent de $\{0\}$, alors $L = [L,L]$, et $L^{(2)} = [L^{(1)},L^{(1)}] = L$, $\cdots$ , $L^{(i)} = [L^{(i-1)},L^{(i-1)}]=L $, i.e. $\forall n \in \NN $  $L^{(n)}\ne \{0\}$.
\end{itemize}
\end{rmq}

\begin{exe}
\quad Regardons l'algèbre des matrices triangulaires supérieures $\ttt(n,\KK)$. Une base de $\ttt(n,\KK)$ est $\{ e_{ij} \mid 1\leq i \leq j\leq n \}$.

Pour montrer que $L = \ttt(n,\KK)$  est résoluble on calcul explicitement sa série dérivée, en utilisant la formule $ [ e_{ij},e_{kl} ]  = \delta_{jk}e_{il}  -  \delta_{li}e_{kj} $ $(\star)$. En particulier, nous avons:  $[ e_{ii},e_{ij} ]  = e_{ij}$ pour $i<j$. Ce qui montre que  $ \nn(n,\KK) \subset [L,L] $. De plus, comme $\ttt(n,\KK) = \dd(n,\KK) \oplus \nn(n,\KK)$ et que $\dd(n,\KK)$ est abélienne, on conclut que $ \nn(n,\KK) = [L,L]$.

Reste à monter que la sous-algèbre $\nn(n,\KK)$ est résoluble. Avant cela définisons le \textbf{niveau} d'un élément $e_{ij}$ comme l'entier $j-i$.

Pour cela, regardons l'action du crochet sur deux éléments de la base de $\nn(n,\KK)$, $e_{ij}$ et $e_{kl}$ avec $i<j$ et $k<l$. On peut aussi supposer, sans perdre d'éléments, que $i \neq l$. On a alors:
\begin{center}
$[e_{ij},e_{kl}] = \left \{
\begin{aligned}
e_{il} &\quad si \quad j = k \\
0 &\quad sinon
\end{aligned}
\right. $
\end{center}
Le niveau de $e_{il}$ est égal à la somme des niveaux de $e_{ij}$ et de $e_{kl}$. Ainsi, le niveau du crochet de deux éléments de niveau $1$ sera $2$. On a donc que $L^{(2)}$ est engendré par les éléments $e_{ij}$ avec $j-i \geq 2$. Par réccurence on obtient que $L^{(i)}$ est engendré par les éléments $e_{ij}$ avec $j-i \geq 2^{i-1}$. Finalement, il est clair que $L^{(i)} = \{0\}$ quand $2^{i-1} > n-1$. Ainsi $\nn(n, \KK)$ est résoluble. 

Donc $\ttt(n, \KK)$ est également résoluble. 

En particulier, $\ttt(n, \KK)$ et $\nn(n,\KK)$ ne sont pas simple.
\end{exe}

\begin{prop}\label{prop:solv}
Soit $L$ une algèbre de Lie: 
\begin{itemize}
\item[(a)] Si $L$ une algèbre résoluble, alors toutes ses sous-algèbres et les images de ses homomorphismes sont résolubles.
\item[(b)] Si $I$ est un idéal résoluble de $L$ tel que $L/I$ est résoluble alors $L$ aussi est résoluble.
\item[(c)] Si $I$ et $J$ sont deux idéaux résolubles de $L$, alors $I+J$ est résoluble.
\end{itemize}
\end{prop}

\begin{prv}
\begin{enumerate}
\item[(a)] Par définition, si $K$ est une sous algèbre de $L$, alors $K^{(i)} \subset L^{(i)}$, de même si $\phi: L \rightarrow M$ est épimorphisme une récurrence sur $i$ montre que $\phi\left(L^{(i)}\right)=M^{(i)}$.

\item[(b)] Posons $(L / I)^{(n)}= \{0\}$, on applique le résultat (a) sur le morphisme canonique : $\pi: L \rightarrow L / I$, on a  $\pi\left(L^{(n)}\right)= \{0\}$, d'où  $L^{(n)} \subset I = ker$ $\pi$. Maintenant si  $I^{(m)}= \{0\}$ le fait évident que $\left(L^{(i)}\right)^{(j)}=L^{(i+j)}$ implique que  $L^{(n+m)}= \{0\}$. 

\item[(c)] Le (d) du théorème (\ref{thm:iso}) donne un isomorphisme entre $(I+J) / J$ et $I /(I \cap J)$. Comme image d'homomorphisme de $I$, $(I+J) / J$  est résoluble donc d'après (b)  $(I+J)$ est résoluble.
\end{enumerate}
\end{prv}

\begin{df}
\quad On dit que $I$ est un idéal \textbf{résoluble maximal} si il n'est inclus dans aucun idéal résoluble. Cet idéal est unique et appelée \textbf{radical} de L, noté $Rad(L)$.
\end{df}

\begin{df}
\quad Si $Rad (L) = \{0\}$ on dit que $L$ est \textbf{semisimple}.
\end{df}

\begin{ex}
\begin{enumerate}
\item Une algèbre de Lie simple est semisimple.
En effet: $L$ est simple donc il n'a que deux idéaux $\{ 0 \}$ et $L$. Or $L$ n'est pas résoluble, donc $Rad (L)= \{0\}$.

\item $L= \{0\}$ est semisimple.
\end{enumerate}
\end{ex}

Notons que pour une algèbre de Lie arbitraire $L$, on a $L/(Rad (L))$ est semisimple (d'après la proposition (\ref{prop:solv}) (b)).

\subsection{Algèbres nilpotentes}

On définit la suite \textbf{centrale descendante} de $L$ par récurrence:
\begin{center}
$L^{1}=L, \quad L^{2}=[L, L]=\left[L, L^{1}\right], \quad L^{3}=\left[L, L^{2}\right], \thinspace \ldots \thinspace, \quad L^{i}=\left[L, L^{i-1}\right]$.
\end{center}

\begin{df}
\quad L'algèbre de Lie $L$ est dite \textbf{nilpotente} si $L^{n} = \{0\}$ pour un certain $n \in \NN^{*}$.
\end{df}

\begin{ex}
\begin{enumerate}
\item On a équivalence entre $L$ abélienne et $L^{2}= \{ 0 \}$. Donc $L$ abélienne $\Rightarrow$ $L$ nilpotente.

\item On clairement $L^{(i)} \subset L^{i}$, $\forall i$, donc les algèbres nilpotentes sont résolubles. Pour illustrer prenons la sous-algèbre des matrices strictement triangulaires supérieures $L = \nn(4,\KK)$. $L$ est engendré par $\{e_{12},e_{13},e_{14},e_{23},e_{24},e_{34}\}$. En procédant comme dans l'exemple de la section (\ref{d}) sur l'algèbre résoluble $\ttt(n, \KK)$, on trouve que $L^{1}$ est engendré par ${e_{13},e_{14},e_{24}}$, et $L^{2}$ est engendré par $\{e_{14}\}$, ainsi que $ L^{3}=0 $. Donc $L$ est nilpotente.
 
\item Une algèbre résoluble n'est cependant pas nécessairement nilpotente. Considérons $L = \ttt(3,\KK)$. Par le même type de calculs que précedament, on a: $L^2 = \nn(3, \KK) = \{ e_{12} \}$. Or $L^3 = [L,L^2]$, donc $[e_{11},e_{12}] = e_{12} \in L^3$. On déduit $L^i = \nn(3, \KK)$, $\forall i \geq 2$. L'algèbre $\ttt(3, \KK)$ n'est donc pas nilpotente.
\end{enumerate} 
\end{ex}

\begin{prop}\label{prop:nilp}
\quad Soit $L$ une algèbre de Lie. Alors :
\begin{itemize}
\item[(a)] Si $L$ est nilpotente, alors ses sous-algèbres et les images de ses homomorphismes sont aussi nilpotentes.

\item[(b)]  Si $L/Z(L)$ est nilpotente, alors $L$ est nilpotente.

\item[(c)] Si $L$ est nilpotente et non nulle, alors $Z(L) \ne \{0\}$.
\end{itemize}
\end{prop}

\begin{prv}
\begin{itemize}
\item[(a)]  Pareil que la preuve pour une algèbre résoluble.
\item[(b)]  Supposons que $L/Z(L)$ est nilpotente. Alors il existe $n \in \NN$ tel que:
\begin{center}
$(L/Z(L))^{n}= \{ 0 \}$.
\end{center} 

Comme $(L/Z(L))^{n}= \{ 0 \} +Z(L)$ et $\pi(L^n) = (L/Z(L))^{n}$, alors:
\begin{center}
$\pi(L^n)= \{0\} +Z(L)$.
\end{center}
Ce qui implique que $L^n \subset Ker(\pi)=Z(L)$.

Ainsi $L^{n+1}=[L,L^n] \subset [L,Z(L)]= \{0\}$. Posons $m = n+1$, on a trouvé $m$ tel que $L^m= \{0\}$. Donc $L$ est nilpotente.

\item[(c)] Le dernier terme non nul dans la série centrale descendante est le centre.
\end{itemize}
\end{prv}

\begin{rmq}
\quad La condition de nilpotence de $L$, peut être reformuler comme suit:

Soit $n \in \NN$ (qui ne dépend que de $L$) $ad$ $x_{1}$ $ad$ $x_{2} \ldots$ $ad$ $x_{n}(y)=0$,  $\forall$ $x_{i}, y \in L$.
En particulier:
$(ad$ $x)^n = 0$, $\forall x \in L$.
\end{rmq}


\chapter{Le théorème d'Engel}

\quad Nous avons vu dans le chapitre précédent la notion d'algèbre nilpotente, que nous pouvons compléter avec la définition suivante:

\begin{df}
\quad Soit $L$ une algèbre de Lie et $x \in L$. On dit que $x$ est \textbf{ad-nilpotent} si $ad$ $x$ est un endomorphisme nilpotent. 
\end{df}

\begin{prop}\label{prop:E1}
\quad Soit $L$ une algèbre de Lie nilpotente, alors $\forall x \in L$, $x$ est ad-nilpotent.
\end{prop}

\begin{prv}
\quad Utiliser la remarque de la page précedente. $\Box$
\end{prv}

La réciproque de cette proposition est le Théorème d'Engel:

\begin{thm}[Théorème d'Engel]\label{thm:engel}
\quad Soit $L$ une algèbre de Lie.
Si tous les éléments de $L$ sont ad-nilpotents, alors $L$ est nilpotente.
\end{thm}

\section{Les Outils}

\quad Avant de démontrer ce théorème nous avons besoin d’introduire quelques résultats supplémentaires.

\begin{lem}\label{lem:E1}
\quad Soit $x \in \gl(V)$ un endomorphisme nilpotent. Alors $ad$ $x$ est aussi nilpotent. 
\end{lem}

\begin{prv}
\quad Nous pouvons associer à l'endomorphisme $x$, les endomorphismes de $End$ $V$ suivant $\lambda_{x}(y)=xy$ et $\rho_{x}(y)=yx$, qui sont respectivement les translations à droite et à gauche de $x$. Ces deux endomorphismes sont nilpotents car $x$ l'est par hypothèse.

De plus, $\lambda_{x}$ et $\rho_{x}$ commutent. Dans tout anneau, la somme ou la différence de deux éléments nilpotents qui commutent sont nilpotentes. Comme on peut écrire :
$ad$ $x(y)= [xy] = xy-yx = \lambda_{x} - \rho_{x}$, on conclut que $ad$ $x$ est nilpotent. $\Box$
\end{prv}

\begin{rmq}
\quad Une matrice peut être ad-nilpotente sans être nilpotente. C'est le cas de la matrice identité. 
\end{rmq}

Nous allons déduire le Théorème d'Engel du résultat qui va suivre. Mais avant rappelons qu'une application linéaire nilpotente a toujours au moins un vecteur propre, associé à son unique valeur propre $0$. C'est le cas $dim$  $L = 1$ du théorème qui suit:

\begin{thm}\label{thm:E1}
\quad Soit $L$ une sous-algèbre de Lie de $\gl (V)$, avec $dim$ $V < + \infty$. Si tous les éléments de $L$ sont nilpotents et $V  \neq  \{0\}$, alors il existe un vecteur $v \in V$, non nul, tel que $L.v = 0$.
\end{thm}

\begin{dem}
\quad On procéde par récurrence sur la dimension de $L$.

Les cas $dim$ $L = 0$ ou $1$ sont évidents.

Supposons que $K \neq \{0\}$ est une sous-algèbre de Lie de $L$. Par le Lemme (\ref{lem:E1}) $K$ agit (via $ad$) comme une algèbre de Lie, d'applications linéaires nilpotentes, dans l'espace vectoriel $L$, d'où sur l'espace vectoriel $L/K$. Comme $dim$ $K < dim$ $L$, l'hypothèse de récurrence assure l’existence d'un vecteur $x + K \neq K $ dans $L/K$, qui est annulé par l'image de $K$ dans $\gl(L/K)$, c.à.d $[y,x] \in K$, $\forall y \in K$, tandis que $x \notin K$. En d'autre mot, $K$ est strictement inclus dans $N_{L}(K)$.

Maintenant prenons $K$ une sous-algèbre maximale strictement incluse dans $L$. L'argument précédent force $N_{L}(K) = L$, i.e $K$ est un idéal de $L$. Si $dim$ $L/K$ est supérieur à $1$ alors il existe $\overline{H} \subset L/K$ de dimension $1$, tel que par passage à l'image inverse, on a $K \subset H \subset L$ et $H$ est une sous algèbre strictement incluse dans $L$. Mais par maximalité de $K$, on a $H \subset K$. Contradiction !

On déduit alors que $K$ est de codimension $1$. On peut donc écrire $L=K+ \KK z$ pour $z \in L-K$. Par récurrence, $W = \{ v \in V \mid K.v = 0\}$ est non nul. Puisque $K$ est un idéal, $W$ est stable sous $L$: 
\begin{center}
$x \in L$, $y \in K$, $w \in W$, $\quad yx.w = xy.w - [x,y]w =0$.
\end{center}
\quad On choisit $z \in L-K$ comme ci-dessus, donc l'endomorphisme nilpotent $z$ (dans le sous-espace $W$) a un vecteur propre, i.e. il existe un vecteur $v \in V$ non nul pour lequel $z.v = 0$.

On a finalement, $L.v = 0$ comme désiré. $\Box$
\end{dem}

\section{Démonstration et conséquences}

Nous avons maintenant tout ce qu'il faut pour démontrer le Théorème d'Engel.

\begin{dem}[Théorème d'Engel]
\quad Soit $L$ une algèbre de Lie dont tous les éléments sont ad-nilpotents. Par conséquent $ad$ $L \subset \gl(L)$  satisfait les hypothèses du théorème (\ref{thm:E1}) (on peut supposer que $L \neq \{0 \}$).

Conclusion, il existe $x \neq 0$ dans $L$ pour lequel $[L,x]=\{0\}$, i.e. $Z(L) \neq \{0\}$. Maintenant les éléments de $L/Z(L)$ sont ad-nilpotents et $dim$ $L/Z(L) < dim$ $L$.

Par récurrence sur la dimension de $L$, on  trouve que $L/Z(L)$ est nilpotent. Par le (b) de la proposition (\ref{prop:nilp}), on a que $L$ est aussi nilpotente. $\Box$
\end{dem}

Nous pouvons maintenant déduire un corollaire très utile, même équivalent au théorème d'Engel. Mais avant nous avons besoin d'introduire la définition suivante:

\begin{df}
\quad Soit $V$ un espace vectoriel de dimension finie, un \textbf{drapeau de $V$} est une chaîne de sous-espaces vectoriels tels que:
\begin{center}
$\{0\} = V_{0} \subset V_{1} \subset \cdots \subset V_{n} = V$ et $dim$ $V_{i} = i$, $\forall i$.
\end{center} 

Si $x \in End$ $V$, on dit que $x$ \textbf{stablilise} (ou \textbf{laisse invariant}) le drapeau précédent si:
\center $x.V_{i} \subset V_{i}$, \quad $\forall i$.
\end{df}

\begin{cor}\label{cor:E1}
\quad Soit $L$ une algèbre de Lie dont tous les éléments sont nilpotents. Il existe un drapeau $(V_{i})_{i}$ de $V$ stable dans $L$, i.e. $x.V_{i} \subset V_{i}$, $\forall x \in L$. 

En d'autres mots: il existe une base de $V$ dans laquelle les matrices de $L$ appartiennent à $\nn(n,\KK)$.
\end{cor}

\begin{prv}
\quad On commence avec un vecteur $v \in V$, non nul, annulé par $L$, son existence est assurée par le théorème (\ref{thm:E1}). Posons $V_{1} = \KK v$. Soit $W= V/V_{1}$ et on observe que l'action induite de $L$ dans $W$ est un endomorphisme nilpotent.

Par récurrence sur la dimension de $V$, $W$ a un drapeau stabilisé par $L$, par passage à l'image inverse on obtient le résultat voulu. $\Box$
\end{prv}

Pour finir ce chapitre, évoquons une application  typique du Théorème (\ref{thm:E1}), dont nous aurons besoin plus tard : 

\begin{lem}\label{lem:E2}
\quad Soit $L$ une algèbre de Lie nilpotente, $K$ un idéal de $L$. Alors si $K \neq \{0\}$, $K \cap Z(L) \neq \{ 0 \}$.
En particulier, $Z(L) \neq \{ 0 \}$.
\end{lem}

\begin{prv}
\quad $L$ agit sur $K$, via la représentation adjointe, donc le Théorème (\ref{thm:E1}) donne $x \in K$ non nul, annulé par $L$, i.e. $[L,x]= \{0 \}$. Ainsi $x \in K \cap Z(L)$. $\Box$
\end{prv}



\chapter{Le théorème de Lie}

Dans les chapitres précédent nous avons regardé les algèbres de Lie sur un corps $\KK$ quelconque. Dans la suite nous aurons besoin que $\KK$ soit de caractéristique $0$. En outre pour avoir à disposition les valeurs propres de $ad$ $x$ pour un $x$ quelconque, et non seulement pour $ad$ $x$ nilpotent, nous supposerons aussi que $\KK$ est algébriquement clos. Si tel n'ai pas le cas nous le préciserons.

L'essence du Théorème d'Engel pour une algèbre de Lie nilpotente est l’existence d'un vecteur propre commun pour une algèbre de Lie composé endomorphismes nilpotents (Théorème (\ref{thm:E1})). Le théorème qui va suivre est de nature similaire. Cependant il nécessite une clôture algébrique afin d'assurer que $\KK$ contient toutes les valeurs propres. De même il est nécessaire d'avoir $car$ $\KK =0$.

\begin{thm}\label{thm:L1}
\quad Soit $L$ une sous-algèbre de Lie résoluble de $\gl(V)$, avec $dim$ $V < +\infty$. Si $V \neq \{0\}$, alors $V$ contient un vecteur propre commun pour tous les endomorphismes de $L$.
\end{thm}

\begin{dem}
\quad On utilise une récurrence sur la dimension de $L$.

Le cas $dim$ $L=0$ est trivial.

On tentera d'imiter la preuve du théorème (\ref{prop:E1}), la démonstration suivra donc le schéma suivant:
\begin{itemize}
\item[(1)] Montrer qu'il existe un idéal $K$ de codimension $1$.
\item[(2)] Montrer par récurrence qu'un vecteur propre commun existe pour $K$.
\item[(3)] Vérifier que $L$ stabilise un espace composé de tels vecteurs propres.
\item[(4)] Trouver dans cet espace un vecteur propre qui pour $z \in L$ satisfait $L=K+\KK z$.
\end{itemize}
Commençons maintenant la démonstration :
\begin{itemize}
\item[(1)] Puisque $L$ est résoluble, de dimension positive ($\neq  0$), $L$ contient strictement $[L,L]$. Ainsi $L/[L,L]$ est abélien, d'où tout sous-espace est automatiquement un idéal. On prend un sous-espace de codimension $1$, alors son image inverse $K$ est un idéal de codimension $1$ de $L$ (qui contient $[L,L]$).

\item[(2)] On utilise une récurrence pour trouver un vecteur propre commun $v \in V$ pour $K$. Par la proposition (\ref{prop:solv}) $K$ est résoluble. Si $K = \{ 0 \}$, alors $L$ est abélien de dimension 1 et un vecteur propre pour une base de $L$ finit la preuve. Dans le cas où $K \neq \{ 0 \}$, l'hypothèse de récurrence fourni, pour $x \in K$, $x.v= \lambda(x)v$, avec $\lambda : K \to \KK$ une application linéaire. On fixe $\lambda$, et on note $W$ le sous-espace:
\begin{center}
$W=\{ w \in V \mid x.w= \lambda(x)w$, $\forall x \in K \} $
\end{center}
On a bien que $W \neq \{0\}$.

\item[(3)] Montrons que $L$ laisse $W$ invariant. Soit $w \in W$ et $x \in L$. Pour déterminer si $x.w$ vit dans $W$, prenons $y \in K$ et examinons :
\begin{center}
$yx.w=xy.w-[x,y].w= \lambda(y)x.w- \lambda([x,y])w $
\end{center}
Ainsi il faut montrer que $\lambda([x,y])=0$. Pour cela fixons $w \in W$ et $x \in L$. Soit $n>0$ le plus petit entier pour lequel $w$, $x.w, \cdots$, $x^{n}.w$ sont linéairement indépendant. Notons $W_{i}$ le sous-espace engendré par $w$, $\cdots$, $x^{i-1}.w$, donc $dim$ $W_{n}=n$ et $W_{n}=W_{n+i}$ ($i\geq 0$) et $x$ est une application de $W_{n}$ dans $W_{n}$. Il est facile de vérifier que chaque $y \in K$ laisse chacun des $W_{i}$ invariant. Relativement à la base $w$, $x.w$, $\cdots$, $x^{n}.w$, on affirme que $y \in K$ est représenté par une matrice triangulaire supérieure dont les éléments diagonaux sont égaux à $\lambda(y)$. Cela vient de la congruence:
\begin{center}
$(\star) \quad yx^{i}.w = \lambda(y)x^{i}w \quad (mod$ $W_{i})$
\end{center}
que l'on prouve par récurrence sur $i$. Le cas $i=0$ est évident. Écrivons: 
\begin{center}
$yx^{i}.w = yxx^{i-1}.w = xyx^{i-1}.w -[x,y]x^{i-1}.w $
\end{center}
Par récurrence, $yx^{i-1}.w = \lambda(y)x^{i-1}.w+w'$ ($w' \in W_{i-1}$). Puisque $x$ est une application de $W_{i-1}$ dans $W_{i}$, par construction, ($\star$) est donc vraie pour tout i.

De notre description de l'action de $y \in K$ sur $W_{n}$ on a $tr_{W_{n}}(y)=n \lambda(y)$. En particulier, ceci est vrai pour les éléments de $K$ de la forme $[x,y]$. Comme $x$, $y$ stabilise $W_{n}$, on a que $[x,y]$ agit sur $W_{n}$ comme le commutateur de deux endomorphismes de $W_{n}$. Leurs trace est par conséquent nulle. On conclut que $n \lambda([x,y]) = 0$. Du fait que $car$ $\KK=0$, on a $\lambda([x,y])=0$, comme demandé.

\item[(4)] On écrit $L=K+ \KK z$ et on utilise le fait que $\KK$ est une clôture algébrique pour trouver un vecteur propre $v_{0} \in W$ de $z$ (pour une des valeurs propres de $z$). Alors $v_{0}$ est évidement un vecteur propre pour $L$ et $\lambda$ peut être étendu en une application linéaire de $L$ telle que $x.v_{0} = \lambda(x)v_{0}$, $x \in L$. $\Box$
\end{itemize}
\end{dem}

\begin{cor}[Théorème de Lie]\label{cor:L1}
\quad Soit $L$ une sous-algèbre de Lie résoluble de $\gl(V)$, avec $dim$ $V = n \in \NN$. Alors $L$ stabilise un drapeau de $V$.
En d'autres mots, les matrices de $L$, dans une base de $V$ bien choisie, sont triangulaires supérieures.
\end{cor}

\begin{prv}
\quad On utilise le théorème (\ref{thm:L1}), ainsi qu'une récurrence sur la dimension de $V$. $\Box$
\end{prv}

Plus généralement, considèrons $L$ une algèbre de Lie résoluble, et $\phi$: $L \to \gl(V)$ une représentation de dimension finie de $L$. Alors $\phi(L)$ est résoluble, par la proposition (\ref{prop:solv})(a), donc stabilise un drapeau de $V$ par le corollaire (\ref{cor:L1}).

De plus, si $\phi$ est la représentation adjointe, un drapeau de sous-espaces stables par $L$ est juste une chaîne d'idéaux de $L$, chacuns de codimension $1$.

On en déduit le corollaire suivant: 

\begin{cor}\label{cor:L2}
\quad Soit $L$ résoluble. Alors il existe une chaîne d'idéaux de $L$ tels que:
\center $\{0\}=L_{0} \subset L_{1} \subset \cdots \subset L_{n} = L$ et $dim$ $L_{i}=i$.
\end{cor}

\begin{cor}\label{cor:L3}
\quad Soit $L$ une algèbre de Lie résoluble. Alors $x \in [L,L]$ implique que $ad_{L}$ $x$ est nilpotent.
En particulier, $[L,L]$ est nilpotente.
\end{cor}

\begin{prv}[Corollaire \ref{cor:L3}]
\quad On trouve un drapeau d'idéaux comme dans le corollaire (\ref{cor:L2}). On considère la base $x_{1}$, $\cdots$, $x_{n}$ de $L$, telle que $x_{1}$, $\cdots$, $x_{i}$ engendre $L_{i}$. Dans cette base la matrice de $ad$ $L$ vie dans $\ttt(n,\KK)$. Par conséquence les matrices de $[ad$ $L$,$ad$ $L]$ sont dans $\nn(n,\KK)$, l'algèbre dérivée de $\ttt(n,\KK)$. Il en suit que $ad_{L}$ $x$ est nilpotent pour $x \in [L,L]$, a fortiori $ad_{[L,L]}$ $x$ est nilpotent, alors $[L,L]$ est nilpotent par le théorème d'Engel (\ref{thm:engel}). $\Box$
\end{prv}

Au début de ce chapitre nous avons souligné l'importance de $car$ $\KK = 0$, nous allons maintenant donner un exemple du cas $car$ $\KK \neq 0$:

\begin{exe}
\quad Dans cet exemple le corps sera $\FF_3 = \ZZ /3 \ZZ$. On a alors $car$ $\FF_3 = 3$. Considérons les matrice de $\gl(3,\FF_3)$ suivantes: 
\begin{center}
$x = \begin{pmatrix}
0 & 1 & 0 \\
0 & 0 & 1 \\
1 & 0 & 0 
\end{pmatrix}$, \quad
$y = \begin{pmatrix}
0 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 2 
\end{pmatrix}$
\end{center}
\begin{align*}
\text{On a:} \qquad [x,y] & = xy - yx \\
 & = \begin{pmatrix} 
0 & 1 & 0 \\
0 & 0 & 2 \\
0 & 0 & 0 
\end{pmatrix}-\begin{pmatrix}
0 & 0 & 0 \\
0 & 0 & 1 \\
2 & 0 & 0 
\end{pmatrix}\\ & = \begin{pmatrix}
0 & 1 & 0 \\
0 & 0 & 1 \\
-2 & 0 & 0 
\end{pmatrix} \\ & = x \quad 
(\text{car } \overline{1} = -2 \text{ dans } \FF_3).
\end{align*}

Montrons que l'espace $L$ engendré par $x$ et $y$ est une sous-algèbre de Lie résoluble. Soient $z_1,z_2 \in L$, tels que $z_1 = a_1x+b_1y$ et $z_2=a_2x+b_2y$ avec $a_i,b_i \in \FF_3$. On a:
\begin{align*}
[z_1,z_2] &= [a_1x+b_1y,a_2x+b_y]\\
& = a_1a_2[x,x]+a_1b_2[x,y]+b_2a_1[y,x]+b_1b_2[y,y]\\
& = (a_1b_2-b_2a_1)[x,y]\\
& = (a_1b_2-b_2a_1)x \in L
\end{align*}

$L$ est bien une sous-algèbre de Lie. De plus comme $[x,y]=x$, on a que $[L,L]$ est engendré par $x$. Ainsi $[L,L]$ est abélienne, donc $L$ résoluble.

\bigskip
Pour la matrice $y$ il est facile de trouver des vecteurs propres.

Posons $e_1 = \begin{pmatrix}1 \\ 0 \\ 0\end{pmatrix}$, $e_2 = \begin{pmatrix}0 \\ 1 \\ 0 \end{pmatrix}$ et $e_3 = \begin{pmatrix}0 \\ 0 \\ 1\end{pmatrix}$. Alors $e_1$, $e_2$ et $e_3$ sont des vecteurs propres de $y$ respectivement associés aux valeurs propres $0$, $1$ et $2$.

Regardons si $x$ a un vecteur propre commun avec $y$:
\begin{center}
$xe_1 = e_3$, \quad $xe_2=e_1$, \quad $xe_3=e_2$
\end{center}

Donc aucun de ces trois vecteurs n'est vecteur propre de $x$. 

La conclusion du théorème de Lie est donc fausse dans ce cas. 
\end{exe}


\chapter{Critère de Cartan}

Il serait utile d'avoir un critère pour étudier si une algèbre de Lie est résoluble. C'est exactement ce qui va nous intéresser dans cette partie. 

\section{Les outils}

\subsection{La décomposition de Jordan-Chevalley}

Dans cette sous-section uniquement, on considère $car$ $\KK$ quelconque. Nous sortons un peu du cadre des algèbre de Lie pour introduire un outil très utile dans l’étude des applications linéaires. Rappelons la forme de Jordan pour un endomorphisme $x$ sur un corps algébriquement clos. Sous forme matricielle c'est une matrice diagonales par blocs avec des blocs de la forme :
\begin{center}
$
\begin{pmatrix}
a & 1 & \cdots & 0 \\
\vdots & \ddots & \ddots & \vdots \\
 0 & \cdots & a & 1 \\
0 & \cdots & 0 & a
\end{pmatrix}
$ avec $a$ valeur propre de $x$.
\end{center}

Puisque $diag(a, \cdots, a)$ commute avec une matrice nilpotente qui a des $1$ sur la surdiagonale et des $0$ partout ailleurs, $x$ est la "somme" de matrices diagonales et de matrices nilpotentes. Nous pouvons rendre cette décomposition plus précise, comme suit. 

\begin{df}
\quad Soit $V$ un $\KK$-espace vectoriel de dimension finie. On appel $x \in End(V)$ \textbf{semisimple} si les racines de son polynôme minimal sur $\KK$ sont toutes distinctes. 
\end{df}

\begin{prop}
\quad $x \in End(V)$ semisimple $\iff x$ est diagonalisable. 
\end{prop}

\begin{rmq}
\quad La somme ou la différence de deux endomorphismes semisimples est encore semisimple. De même que la restriction à un sous-espace sera encore semisimple.
\end{rmq}

\begin{prop}\label{prop:C1}
\quad Soient $V$ un espace vectoriel de dimension finie sur $\KK$ et $x \in End(V)$ :
\begin{itemize}
\item[(a)] Il existe un unique couple $x_{s},x_{n} \in End(V)$ qui satisfait :
\begin{center}
$x=x_{s}+x_{n}$ avec $x_{s}$ semisimple, $x_{n}$ nilpotent et $x_{s},x_{n}$ commutent.
\end{center}

\item[(b)] Il existe deux polynômes $p(T)$, $q(T)$ de $\KK[T]$, sans terme constant, tels que $x_{s}=p(x)$, $x_{n} = q(x)$. 

En particulier, $x_{s}$ et $x_{n}$ commutent avec tout endomorphisme qui commute avec $x$.

\item[(c)] Si $A \subset B \subset V$ des sous-espaces, et $x$ une application de $B$ dans $A$, alors $x_{s}$ et $x_{n}$ sont également des applications de $B$ dans $A$. 
\end{itemize}
\end{prop}

\begin{df}
\quad La décomposition $x = x_{s}+x_{n}$ est appelé \textbf{décomposition de Jordan-Chevalley} de $x$, ou juste \textbf{décomposition de Jordan}. Les endomorphismes $x_{s}$ et $x_{n}$ sont respectivement appelés \textbf{partie semisimple} et \textbf{partie nilpotente} de $x$.
\end{df}

\begin{prv}
\quad Soient $a_{1},\cdots,a_{k}$ (avec multiplicité $m_{1},\cdots,m_{k}$) les valeurs propres distinctes de $x$, donc le polynôme caractéristique de $x$ est $\prod\limits_{i=0}^k  \left( T-a_{i} \right)^{m_{i}}$. Si $V_{i}=Ker$ $(x-a_{i}.I_{n})^{m_{i}}$, alors $V$ est la somme direct des sous-espaces $V_{1}, \cdots, V_{k}$, qui sont tous stable par $x$. Dans $V_{i}$, $x$ a clairement $(T-a_{i})^{m_{i}}$ comme polynôme caractéristique.

Maintenant on applique le théorème des restes Chinois (pour l'anneau $\KK [T]$) pour trouver le polynôme $p(T)$ qui satisfait les congruences suivantes :
\begin{center}
$
\left\{
\begin{aligned}
p(T) & \equiv a_{i} \quad (mod \enskip (T-a_{i})^{m_{i}}) \quad (\star) \\
p(T) & \equiv 0 \quad (mod \enskip T)
\end{aligned}
\right. $
\end{center}

Notons que la dernière condition est superflue si $0$ est valeur propre de $x$, alors qu'autrement $T$ est premier avec les autres polynômes. 

Fixons $q(T)=T-p(T)$. Il est évident que $p(T)$ et $q(T)$ n'ont pas de terme constant, puisque $p(T) \equiv 0 \quad (mod \enskip T)$.

Nomons $x_{s} = p(x)$ et $x_{n} = q(x)$. Puisque ce sont des polynômes en $x$, $x_{s}$ et $x_{n}$ commutent entre eux, mais aussi avec les endomorphismes qui commutent avec $x$. Ils stabilisent tous les sous-espaces de $V$ stables par $x$, en particulier les $V_{i}$. Les congruences $(\star)$ montrent que la restriction de $x_{s}-a_{i}.I_{n}$ a $V_{i}$ est $\{0\}$, $\forall i$, par conséquent $x_{s}$ est diagonal sur $V_{i}$ avec pour valeurs propres $a_{i}$.

Par définition, $x_{n} = x - x_{s}$, il est donc clair que $x_{n}$ est nilpotent. Du fait que $p(T)$ et $q(T)$ n'ont pas de terme constant (c) est évident à ce stade. 

Il reste a prouver l'unicité de l'assertion (a). Soit $x=s+n$ une autre décomposition, on a $x_{s}-s=n-x_{n}$. Par (b), tous les endomorphismes en vue commutent. Comme une somme d'endomorphismes semisimples (resp. nilpotents) qui commutent est encore semisimples (resp. nilpotents). On déduit que $x_{s}-s=n-x_{n}=0$ , car il n'y a que l'endomorphisme nul qui soit à la fois semisimple et nilpotent. Ainsi $x_{s}=s$ et $n =x_{n}$. $\Box$
\end{prv}

Pour préciser pourquoi la décomposition de Jordan est un outils précieux, regardons un cas particulier :

\begin{ex}
\quad Considérons la représentation adjointe de l'algèbre de Lie $\gl(V)$, $dim$ $V < +\infty$. Si $x \in \gl(V)$ est nilpotent alors $ad$ $x$ aussi (lemme (\ref{lem:E2})).

Similairement, si $x$ est semisimple, alors $ad$ $x$ l'est aussi. Vérifions cela: 

Choisissons une base $(v_{1},\cdots,v_{n})$ de $V$ telle que la matrice de $x$ soit $diag(a_{1},\cdots,a_{n})$. Soit $\{ e_{ij} \}$ la base canonique de $\gl(V)$ par rapport à notre base de $V$, i.e. $e_{ij}(v_{k})=\delta_{jk}v_{i}$. Alors un calcul rapide (voir formule $(\star)$ dans de la section (\ref{b})) montre que $ad$ $x(e_{ij})=(a_{i}-a_{j})e_{ij}$, donc $ad$ $x$ est une matrice diagonale, dans une base bien choisie de $\gl(V)$. 
\end{ex}

\begin{lem}\label{lem:C1}
\quad Soit $x \in End$ $V$ ($dim$ $V < +\infty$), $x=x_{s}+x_{n}$ la décomposition de Jordan. Alors $ad$ $x = ad$ $x_{s}+ad$ $x_{n}$ est la décomposition de Jordan de $ad$ $x$ dans $End(End$ $V)$. 
\end{lem}

\begin{prv}
\quad Nous avons vu que $ad$ $x_{s}$ et $ad$ $x_{n}$ sont respectivement semisimple et nilpotent. Ils commutent, puisque $[ad$ $x_{s},ad$ $x_{n}] = ad$ $[x_{s},x_{n}]=0$. Alors le (a) de la proposition (\ref{prop:C1}) s'applique. $\Box$
\end{prv}

\subsection{Critère de nilpotence d'un endomorphisme}\label{a}

Nous pouvons obtenir un critère puissant en se basant sur la trace de certains endomorphismes de $L$. Il est évident que $L$ va être résoluble si $[L,L]$ est nilpotente (c'est la réciproque du corollaire (\ref{cor:L3})).

Le théorème d'Engel (\ref{thm:engel}) nous dit que $[L,L]$ est nilpotente si (et seulement si) tout $ad_{[L,L]}$ $x$, $x \in [L,L]$, est nilpotent. Nous commencerons avec un critère sur la "trace" pour la nilpotence d'un endomorphisme.

\begin{lem}\label{lem:C2}
\quad Soit $A \subset B$ deux sous-espaces de $\gl(V)$ ($dim$ $V<+\infty$). Soit $M= \{ x \in \gl(V) \mid [x,B] \subset A \}$. Supposons que $x \in M$ satisfait $tr(xy)=0$, $\forall y \in M$.

Alors $x$ est nilpotent. 
\end{lem}

\begin{prv}
\quad Soit $x=s+n$ la décomposition de Jordan pour $x$. On fixe une base $(v_{1},\cdots,v_{m})$ de $V$ telle que la matrice de $s$ soit $diag(a_{1}, \cdots,a_{m})$. Soit $E$ le sous-espace vectoriel de $\KK$ (par rapport à son sous-corps premier $\QQ$) engendré par les valeurs propres $a_{1},\cdots,a_{m}$ de $s$. On a monté que $s=0$, ou par équivalence, que $E= \{0\}$. Puisque $E$ est de dimension finie sur $\QQ$ (par construction), il est suffisant de montrer que l'espace dual $E^{*}= \{0\}$, i.e. que toute application linéaire $f:$ $E \to \QQ$ est nulle. 

On se donne $f \in E^{*}$, soit $y \in \gl(V)$ dont la matrice dans la base précédente est $diag\left( (f(a_{1}),\cdots,f(a_{m}) \right)$. Si $\{ e_{ij} \}$ est la base correspondante de $\gl(V)$, on a vu dans l'exemple précedent que :
\begin{center}
$ad$ $s(e_{ij}) = (a_{i} - a_{j})e_{ij}$ et $ad$ $y(e_{ij}) = (f(a_{i})-f(a_{j}))e_{ij}$.
\end{center}

Maintenant soit $r(T) \in \KK[T]$ un polynôme sans terme constant qui satisfait $r(a_{i}-a_{j}) = f(a_{i})-f(a_{j})$, $\forall (i,j)$. L’existence d'un tel polynôme est donné par l’interpolation de Lagrange, il n'y pas d’ambiguité sur les variables puisque $a_{i} - a_{j} = a_{k} -a_{l}$ implique (par linéarité de $f$) que $f(a_{i}) - f(a_{j}) = f(a_{k}) -f(a_{l})$. On a donc $ad$ $y= r(ad$ $s)$.

Maintenant, $ad$ $s$ est la partie semisimple de $ad$ $x$, par le lemme (\ref{lem:C1}), donc on peut l'écrire comme un polynôme en $ad$ $x$ sans terme constant, par la propriété (\ref{prop:C1}). Ainsi $ad$ $y$ est aussi un polynôme en $ad$ $x$ sans terme constant. Par hypothèse, $ad$ $x$ est une application de $B$ dans $A$, donc on a aussi $ad$ $y(B) \subset A$, i.e. $y \in M$.

On utilise l'hypothèse $tr(xy)=0$, on a $\sum a_{i}f(a_{i}) = 0$. Le côté gauche est une combinaison $\QQ$-linéaire d'éléments de $\KK$, en appliquant $f$ on obtient : $\sum f(a_{i})^2 = 0$. Comme les $f(a_{i}) \in \QQ$, on a $f(a_{i})=0$, $\forall i$. Finalement $f$ doit être identiquement nulle, parce que les $a_{i}$ engendre $E$. Ce qui conclu cette preuve. $\Box$
\end{prv}

\section{Critère de Cartan et conséquences}\label{c}

Avant d'énoncer le Critère de Cartan, rappelons une identité qui nous sera utile dans la suite : 
\begin{center}
Si $x,y,z$ des endomorphismes d'espace vectoriel de dimension finie, alors $tr([x,y]z)=tr(x[y,z])$ $(\star)$
\end{center}

\begin{thm}[Critère de Cartan]\label{cartan}
Soit $L$ une sous-algèbre de $\gl(V)$, avec $dim$ $V<+\infty$. On suppose que $tr(xy)=0$, $\forall x \in [L,L]$, $\forall y \in L$.

Alors $L$ est résoluble. 
\end{thm}

\begin{dem}
\quad Comme remarqué au début du (\ref{a}), il est suffisant de montrer que $[L,L]$ est nilpotent, ou juste que tout $x \in [L,L]$ est un endomorphisme nilpotent (lemme (\ref{lem:E1}) et théorème d'Engel). Pour cela nous appliquons le lemme (\ref{lem:C2}) à la situation : $V$ comme donné, $A=[L,L]$, $B=L$.

Soit $M= \{ x \in \gl(V) \mid [x,L] \subset [L,L] \}$. Il est évident que $L \subset M$. Notre hypothèse est que $Ttr(xy)=0$, $\forall x \in [L,L]$, $\forall y \in L$, alors que pour conclure avec le lemme (\ref{lem:C2}) on a besoin d'un argument plus fort: \quad $tr(xy)=0$ pour $x \in [L,L]$, $y \in M$.

Maintenant si $x,y \in L$ et si $z \in M$, alors l'identité $(\star)$ montre que:
\begin{center}
$tr([x,y]z)=tr(x[y,z])=tr([y,z]x)$
\end{center}

Par définition de $M$, $[y,z] \in [L,L]$, d'où $tr([y,z]x)=0$ par hypothèse. $\Box$
\end{dem}

\begin{cor}
\quad Soit $L$ une algèbre de Lie telle que $tr(ad$ $x$ $ad$ $y) = 0$, $\forall x \in [L,L]$, $\forall y \in  L$.

Alors $L$ est résoluble.
\end{cor}

\begin{prv}
\quad On applique le théorème à la représentation adjointe de $L$, et on obtient que $ad$ $L$ est résoluble. Puisque $Ker$ $ad = Z(L)$ est résoluble, $L$ est lui-même est résoluble (proposition (\ref{prop:solv})). $\Box$
\end{prv}


\chapter{Forme de Killing et algèbres de Lie Semisimples}

\begin{df}
\quad Soit $L$ une algèbre de Lie quelconque. On n'appel \textbf{forme de Killing} la forme bilinéaire symétrique définie par:
\begin{align*}
\kappa : L \times L & \longrightarrow \KK \\
(x,y) & \longmapsto tr(ad \enskip x \enskip ad \enskip y)
\end{align*}
\end{df}

\begin{prop}
\quad $\kappa$ est associative, au sens où $\kappa([x,y],z)=\kappa(x,[y,z])$.
\end{prop}

\begin{prv}
En utilisant l'identité $tr([x,y]z)=tr(x[y,z])$ le résultat est évident. $\Box$
\end{prv}

Le lemme qui va suivre aurait pu être utile si nous avions poussé plus loin ce travaille de recherche. 

\begin{lem}
\quad Soit $I$ un idéal de $L$. Si $\kappa$ la forme de Killing de $L$ et $\kappa_I$ la forme de Killing de $I$ (vue comme une algèbre de Lie), alors $\kappa_I = \kappa_{\mid I \times I}$.
\end{lem}

\begin{prv}
\quad Rapelons un fait d'algèbre linéaire: Si $W$ un sous-espace de $V$ de dimension fini, et $\phi$ un endomorphisme de $V$, tel que $Im$ $\phi \subset W$, alors $tr(\phi) = tr(\phi_{\mid W})$. Pour prouver cela, il faut étendre une base de $W$ en une base de $V$ et regarder la matrice de $\phi$ dans cette base. Maintenant si $x,y \in I$, alors $(ad$ $x)(ad$ $y)$ est un endomorphisme de $L$, application de $L$ dans $I$, donc $(ad$ $x)(ad$ $y)_{\mid I}=(ad_I$ $x)(ad_I$ $y)$. $\Box$
\end{prv}

En général, une forme bilinéaire symétrique $\beta(x,y)$ est appeler non-dégénéré si son \textit{noyau} ou \textit{radical} $S$ est $\{0\}$, où $S = \{ x \in L \mid \beta(x,y)=0$, $\forall y \in L \}$. Du fait que la forme de Killing est associative, son noyaux est plus qu'un simple sous-espace: 
\begin{center}
$S$ est un idéal de $L$. 
\end{center}

Pour les algèbres linéaires, un moyen pratique pour tester la non dégénérecence suit: 

On fixe une base $x_1, \cdots, x_n$ de $L$. Alors $\kappa$ est non dégénérée si et seulement si la matrice de taille $n \times n$, telle que son entré $i,j$ est égale à $\kappa(x_i,x_j)$, est de déterminant non nul. 

\begin{exe}
\quad Calculons la forme de Killing de $\ssl(2,\KK)$, pour cela utilisons la base cannonique donné dans le paragraphe (\ref{b}), nous changerons seulement l'ordre $(x,h,y)$. On a :
\begin{center}
$ad$ $x = \begin{pmatrix} 2 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & -2 \end{pmatrix}$,
$ad$ $h = \begin{pmatrix} 0 & -2 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{pmatrix}$,
$ad$ $y = \begin{pmatrix} 0 & 0 & 0 \\ -1 & 0 & 0 \\ 0 & 2 & 0 \end{pmatrix}$.
\end{center} 

Par consequent la matrice de $\kappa$ est: 
$\begin{pmatrix} 
0 & 0 & 4 \\
0 & 8 & 0 \\
4 & 0 & 0
\end{pmatrix}$,et son déterminant est $-128$, ainsi $\kappa$ est non dégénéré. (Ceci est vrai tant que $car$ $\KK \neq 2$.)
\end{exe}

Rappelons qu'une algèbre de Lie est semisimple si $Rad(L)= \{0\}$. 

\begin{prop}
\quad $L$ est semisimple si et seulement si le seul idéal abélien de $L$ est $\{0\}$.
\end{prop}

\begin{prv}
\quad En effet, un idéal abélien sera automatiquement résoluble, donc inclus dans $Rad(L)$. Et réciproquement, si $Rad(L) \neq \{0\}$ il est inclue $Rad(L)$, le dernier terme non nul de la série dérivée sera un idéal abélien.
\end{prv}

\begin{thm}
\quad Soit $L$ une algèbre de Lie. Alors $L$ est semisimple si et seulement si sa forme de Killing est non dégénérée. 
\end{thm}

\begin{dem}
\quad Suposons d'abord que $Rad(L) = \{0\}$. Soit $S$ le noyau de $\kappa$. Par définition, $tr(ad$ $x$ $ad$ $y)=0$, $\forall x \in S,y \in L$ (en particulier pour $y \in [L,L]$). Selon le critère de Cartan (\ref{cartan}), $ad_L$ $S$ est résoluble, par consequent $S$ est résoluble. Comme remarqué plus haut $S$ est un idéal de $L$, donc $S \subset Rad(L) = \{0\}$. Ainsi $\kappa$ est non dégénérée. 

Réciproquement, supposons que $S = \{0 \}$. Pour montrer que $L$ est semisimple, il suffit de prouver que tout idéal abélien $I$ de $L$ est inclus dans $S$. Soient $x \in I$ et $y \in L$. Alors $ad$ $x$ $ad$ $y$ application de $L \to I$, et $(ad$ $x$ $ad$ $y)^2$ une application de $L$ dans $[I,I] =\{0\}$. Ceci nous dit que $ad$ $x$ $ad$ $y$ est nilpotent, donc que $0 = tr(ad$ $x$ $ad$ $y)= \kappa(x,y)$. Ainsi $I \subset S = \{0\}$. $\Box$
\end{dem}

\begin{rmq}
\quad La deuxième partie de la preuve reste valide  si la caractéristique de $\KK$ est positive.
\end{rmq}

Pour finir ce mémoire, nous soulignerons que cette démonstration montrer que nous avons toujours $S \subset Rad(L)$. 

\end{document}
